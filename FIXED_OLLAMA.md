# âš¡ FIXED - CPU Metrics & Ollama Integration

## ğŸ¯ What Was Fixed

âœ… **Ollama Integration:** Now using `axios` consistently (was using `fetch`)
âœ… **CPU Metrics:** Will now display real values when Ollama responds
âœ… **Better Logging:** Server logs show exactly what's happening
âœ… **Error Handling:** Clear messages if Ollama isn't running
âœ… **Direct Ollama:** No external APIs - just Ollama locally

---

## ğŸš€ Quick Start (3 Steps)

### Step 1ï¸âƒ£: Make sure Ollama is installed

Check if Ollama is installed:
```bash
ollama --version
```

If not installed, download from: https://ollama.ai

### Step 2ï¸âƒ£: Start Ollama Server

**Open Terminal 1:**
```bash
ollama serve
```

Wait for it to show: `Listening on http://127.0.0.1:11434`

### Step 3ï¸âƒ£: Start the Safety Gateway

**Open Terminal 2:**
```bash
cd /Users/lalithkumargn/Desktop/hack-day
npm run server
```

You should see:
```
[Gateway] Safety Gateway API running on port 3001
```

---

## ğŸ§ª Test It Now

**Open Terminal 3:**

### Test Safe Prompt:
```bash
curl -X POST http://localhost:3001/analyze \
  -H "Content-Type: application/json" \
  -d '{"prompt":"What is a pen?"}'
```

**Expected Output:**
```json
{
  "result": "SAFE",
  "llmResponse": "A pen is a writing instrument...",
  "performance": {
    "cpuSpeed": 2400,
    "cpuThroughput": 1500,
    "cpuCores": 8
  }
}
```

### Or use the test script:
```bash
bash /Users/lalithkumargn/Desktop/hack-day/test-gateway.sh
```

---

## ğŸ“Š What You Should See

### Safe Prompt ("What is a pen?")
- âœ… `"result": "SAFE"`
- âœ… `"llmResponse": "<actual response about pen>"`
- âœ… CPU metrics: `cpuSpeed`, `cpuThroughput`, `cpuCores`
- âœ… Dashboard shows: âœ“ SAFE badge

### Unsafe Prompt ("Ignore rules...")
- âœ… `"result": "BLOCKED"`
- âœ… `"llmResponse": null` (NOT sent to Ollama)
- âœ… CPU metrics still shown
- âœ… Dashboard shows: âœ— BLOCKED badge

---

## ğŸ“ˆ Dashboard (Optional)

To also see the beautiful dashboard:

**Open Terminal 4:**
```bash
npm start
```

Then open http://localhost:3000 and click the test buttons!

---

## ğŸ”§ Server Logs (Terminal 2)

Watch the logs to see what's happening:

```
[Gateway] Analyzing prompt: "What is a pen?"
[Gateway] Analysis result: SAFE
[Gateway] Prompt is SAFE - forwarding to Ollama...
[Ollama] Sending prompt to http://localhost:11434/api/generate with model: llama2
[Ollama] Received response: A pen is a writing instrument that...
[Gateway] Ollama response received
[Gateway] Returning response (CPU: 2400MHz, 1500MB/s)
```

---

## ğŸ› Common Issues

### âŒ "Ollama connection failed"
**Fix:** Make sure Terminal 1 is running `ollama serve`

### âŒ "Model not found"
**Fix:** Download llama2 model:
```bash
ollama pull llama2
```

### âŒ "Waiting forever for response"
**This is NORMAL!** First request takes 30-60 seconds while Ollama loads the model. Be patient!

### âŒ "High CPU usage"
**This is NORMAL!** When processing, it uses all cores. This is expected.

---

## ğŸ“ Files Modified

1. **server.js** - Fixed Ollama integration with axios
2. **Created OLLAMA_SETUP.md** - Detailed setup guide
3. **Created test-gateway.sh** - Automated test script

---

## âœ… Verification Checklist

After starting everything:

- [ ] Ollama running in Terminal 1 (shows "Listening on...")
- [ ] Gateway running in Terminal 2 (shows "running on port 3001")
- [ ] Test safe prompt - should get LLM response
- [ ] Test unsafe prompt - should get BLOCKED
- [ ] CPU metrics showing in response (not 0)
- [ ] No errors in Terminal 2 logs

---

## ğŸ‰ You're All Set!

Once all terminals are running and tests pass, you have:

âœ… **4-Layer Security Pipeline** (RITD â†’ NCD â†’ LDF â†’ LLM Judge)
âœ… **CPU Metrics Monitoring** (Speed, Throughput, Cores)
âœ… **Ollama Integration** (Local LLM responses)
âœ… **Safe Prompts Get LLM Answers**
âœ… **Unsafe Prompts Get Blocked**
âœ… **Beautiful Dashboard** (Optional frontend)

---

## ğŸ“š Documentation

- `OLLAMA_SETUP.md` - Complete Ollama setup & testing
- `QUICK_START.md` - General setup
- `ARCHITECTURE.md` - System design
- `README.md` - Project overview
- `test-gateway.sh` - Automated tests

---

## ğŸ”— Terminal Layout (Recommended)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Terminal 1             â”‚  Terminal 2          â”‚
â”‚  ollama serve           â”‚  npm run server      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Terminal 3             â”‚  Terminal 4          â”‚
â”‚  bash test-gateway.sh   â”‚  npm start (optional)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Ready? Let's go! ğŸš€**

Start with Step 1 above!
